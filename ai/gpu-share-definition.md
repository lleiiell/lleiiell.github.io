## 白话GPU共享术语

**CUDA**：NVIDIA推出的GPU计算框架, 允许用户在NVIDIA GPU上进行高效并行计算。

**长尾延迟**：作业A密集计算花费1小时, 稀疏计算持续5小时。

**多卡共享**：A、B共用1、2卡

**调度策略**：集中调度, 分散调度

**动态分配**：同一张卡, A用80%, B用20%。动态调整为A用70%,B用30%

**访存密集型**：作业A在某时段对显存访问带宽及延迟非常敏感和依赖。

**GPU内核拦截**：A调用GPU内核, 被自定义动态库拦截

**高优抢占**：算力、显存瓶颈时, 高优作业A可抢占低优作业B资源

**共享安全**：A、B只能访问为其分配的资源，互不冲突、干扰。

**故障隔离**：作业A故障退出时, 同卡作业B正常运行。

**机会作业（Opportunistic Jobs）**：低优先级作业, 可被抢占。

**计算密集型**：作业A在某时段对GPU的算力需求非常大。

**空分复用（Spatial Multiplexing）**：将显卡空间划分为多个区域，区域1、2分别分配给作业A、B使用。

**MIG（Multi-Instance GPU）**：将一张GPU卡划分成7个独立的GPU实例。

**MPS（Multi-Process Service）**：Nvidia推出的多作业共享一张GPU卡技术。

**内核态共享**：劫持Nvidia驱动调用完成资源隔离。

**算力共享**：同一张卡, A用80%, B用20%

**算力隔离**：同一张卡, A、B互斥使用。

**时分复用**：同一张卡, A、B轮流使用时间点1、2。

**弹性资源**：同一张卡, A用80%, B用20%。A用50%时, B可用50%

**拓扑感知**：可用1、2、3卡, 使用1、2卡时运行效率比2、3卡高

**统一内存（UVM）**：内存、显存二合一。申请时数据在内存, 使用时数据在显存。

**显存共享**：单卡16G, A、B共用

**显存隔离**：单卡16G, A上限用6G, B上限用10G

**显存超分**：单卡16G, A、B共用大于16G

**虚拟私有集群（VC）**：A、B共用同一物理集群, 为A、B分别定义虚拟私有集群。

**业务透明**：无缝接入, 无需定制

**用户态共享**：劫持CUDA驱动API调用完成资源隔离。

**资源保障型作业**：高优先级作业, 不会被抢占。

**自省式调度（introspective scheduling）**：将运行中的作业信息反馈给调度器, 便于调度器下一步决策。

**资源碎片**：两台8卡节点, 分别剩余4卡, 不够分配一个6卡需求的作业。


### 相关文档
- https://zhuanlan.zhihu.com/p/285994980
